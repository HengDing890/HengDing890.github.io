<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Heng Ding, hengding@whu.edu.cn"><title>PyTorch实践-(1)回归 · Heng Ding (丁恒)</title><!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="description" content="最近准备从TF转到pytorch(TF做seq2seq的确有些麻烦)，于是想着顺便也把神经网络的东西都撸一遍(敲敲代码记忆深)。
回归可以说是机器学习最基本的内容了，这里我们讲讲线性回归和逻辑回归，并通过pytorch进行编程实践。
线性回归假设$y$表示一个样本点，$x=(f_1, f_2, \d"><meta name="keywords" content="Information Retrieval, Data Mining, Deep Learning"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;border-radius:20px;"><h3 title=""><a href="/">Heng Ding (丁恒)</a></h3><div class="description"><p>科研狗 &amp; 攻城狮 | IR (信息检索) -&gt; AI (人工智能)</p></div></div></div><ul class="social-links"><li><a href="http://weibo.com/5630667316"><i class="fa fa-weibo"></i></a></li><li><a href="http://github.com/HengDing890"><i class="fa fa-github"></i></a></li></ul><div class="footer"></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">博文</a></li><li><a href="/archives">归档</a></li><li><a href="/project">项目</a></li><li><a href="/publication">论文</a></li><li><a href="/about">关于</a></li><li><a href="http://bit.ly/2BxswE6">简历</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>PyTorch实践-(1)回归</a></h3></div><div class="post-content"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
<!-- hexo-inject:begin --><!-- hexo-inject:end --></script>

<p>最近准备从TF转到pytorch(TF做seq2seq的确有些麻烦)，于是想着顺便也把神经网络的东西都撸一遍(敲敲代码记忆深)。</p>
<p>回归可以说是机器学习最基本的内容了，这里我们讲讲线性回归和逻辑回归，并通过pytorch进行编程实践。</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>假设$y$表示一个样本点，$x=(f_1, f_2, \dots, f_n)$表示该样本的特征向量，其中$f_i$表示第$i$个特征值。线性回归的目标在于构建一个数学函数</p>
<p>$h<em>{\theta}(x) = \theta</em>{0} + \theta_{1}{x<em>1} + \dots + \theta</em>{n}{x_n}$</p>
<p>使得$h_{\theta}(x)$的值逼近$y$。</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2017-12-18</span><i class="fa fa-tag"></i><a href="/tags/DL/" title="DL" class="tag">DL </a><a href="/tags/pytorch/" title="pytorch" class="tag">pytorch </a><a href="/tags/深度学习/" title="深度学习" class="tag">深度学习 </a></div></div></div></div><div class="share"><div class="evernote"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"><a href="http://twitter.com/home?status=,http://yoursite.com/child/2017/12/18/PyTorch实践-1-回归/,Heng Ding (丁恒),PyTorch实践-(1)回归,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a role="navigation" href="/2017/08/18/置顶/" title="置顶贴(博文搬迁中,大量干货即将来袭)" class="btn">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>