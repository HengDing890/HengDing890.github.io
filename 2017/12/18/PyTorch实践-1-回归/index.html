<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Heng Ding, hengding@whu.edu.cn"><title>PyTorch实践-(1)回归 · Heng Ding (丁恒)</title><!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="description" content="最近准备从TF转到pytorch(TF做seq2seq的确有些麻烦)，于是想着顺便也把神经网络的东西都撸一遍(敲敲代码记忆深)。
回归可以说是机器学习最基本的内容了，这里我们讲讲线性回归和逻辑回归，并通过pytorch进行编程实践。
线性回归假设(y)表示一个样本点，(x=(f_1, f_2, \d"><meta name="keywords" content="Information Retrieval, Data Mining, Deep Learning"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- hexo-inject:begin --><!-- hexo-inject:end --><link rel="stylesheet" href="/css/prism.css" type="text/css"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;border-radius:20px;"><h3 title=""><a href="/">Heng Ding (丁恒)</a></h3><div class="description"><p>科研狗 &amp; 攻城狮 | IR (信息检索) -&gt; AI (人工智能)</p></div></div></div><ul class="social-links"><li><a href="http://weibo.com/5630667316"><i class="fa fa-weibo"></i></a></li><li><a href="http://github.com/HengDing890"><i class="fa fa-github"></i></a></li></ul><div class="footer"></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">博文</a></li><li><a href="/archives">归档</a></li><li><a href="/project">项目</a></li><li><a href="/publication">论文</a></li><li><a href="/about">关于</a></li><li><a href="http://bit.ly/2BxswE6">简历</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>PyTorch实践-(1)回归</a></h3></div><div class="post-content"><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
<!-- hexo-inject:begin --><!-- hexo-inject:end --></script>

<p>最近准备从TF转到pytorch(TF做seq2seq的确有些麻烦)，于是想着顺便也把神经网络的东西都撸一遍(敲敲代码记忆深)。</p>
<p>回归可以说是机器学习最基本的内容了，这里我们讲讲线性回归和逻辑回归，并通过pytorch进行编程实践。</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>假设(y)表示一个样本点，(x=(f_1, f_2, \dots, f_n))表示该样本的特征向量，其中(f_i)表示第(i)个特征值。线性回归的目标在于构建一个数学函数</p>
<p>$$h_{\theta}(x) = {\theta_0} + {\theta_1}{x_1} + … +{\theta_n}{x_n} $$.</p>
<p>使得( h_{\theta}(x) )的值逼近( y )。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="string">"""</span></div><div class="line">linear_regression.py</div><div class="line"></div><div class="line">本脚本基于pytorch实现线性回归模型</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegressor</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    线性回归器类</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(LinearRegressor, self).__init__()</div><div class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 输入输出均为1维数据</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        前向传播</div><div class="line"></div><div class="line">        :param x: 输入x</div><div class="line">        :return: 预测的输出y</div><div class="line">        """</div><div class="line">        y = self.linear(x)</div><div class="line">        <span class="keyword">return</span> y</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    <span class="comment"># 训练数据</span></div><div class="line">    x_train = np.array([[<span class="number">3.3</span>], [<span class="number">4.4</span>], [<span class="number">5.5</span>], [<span class="number">6.71</span>], [<span class="number">6.93</span>], [<span class="number">4.168</span>],</div><div class="line">                        [<span class="number">9.779</span>], [<span class="number">6.182</span>], [<span class="number">7.59</span>], [<span class="number">2.167</span>], [<span class="number">7.042</span>],</div><div class="line">                        [<span class="number">10.791</span>], [<span class="number">5.313</span>], [<span class="number">7.997</span>], [<span class="number">3.1</span>]], dtype=np.float32)</div><div class="line"></div><div class="line">    y_train = np.array([[<span class="number">1.7</span>], [<span class="number">2.76</span>], [<span class="number">2.09</span>], [<span class="number">3.19</span>], [<span class="number">1.694</span>], [<span class="number">1.573</span>],</div><div class="line">                        [<span class="number">3.366</span>], [<span class="number">2.596</span>], [<span class="number">2.53</span>], [<span class="number">1.221</span>], [<span class="number">2.827</span>],</div><div class="line">                        [<span class="number">3.465</span>], [<span class="number">1.65</span>], [<span class="number">2.904</span>], [<span class="number">1.3</span>]], dtype=np.float32)</div><div class="line"></div><div class="line">    <span class="comment"># 将numpy数据转化为torch数据</span></div><div class="line">    x_train = torch.from_numpy(x_train)</div><div class="line">    y_train = torch.from_numpy(y_train)</div><div class="line"></div><div class="line">    <span class="comment"># 创建线性回归器模型</span></div><div class="line">    model = LinearRegressor()</div><div class="line"></div><div class="line">    <span class="comment"># 定义loss为MSE(均方误差)</span></div><div class="line">    criterion = nn.MSELoss()</div><div class="line"></div><div class="line">    <span class="comment"># 设置SGD为优化函数</span></div><div class="line">    optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 开始训练</span></div><div class="line">    num_epochs = <span class="number">1000</span></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</div><div class="line">        <span class="comment"># pytorch中Variable能够自动进行求导</span></div><div class="line">        <span class="comment"># 因此将数据放入Variable进行包裹</span></div><div class="line">        inputs = Variable(x_train)</div><div class="line">        target = Variable(y_train)</div><div class="line"></div><div class="line">        <span class="comment"># 执行前向传播</span></div><div class="line">        out = model(inputs)</div><div class="line">        <span class="comment"># 计算loss</span></div><div class="line">        loss = criterion(out, target)</div><div class="line"></div><div class="line">        <span class="comment"># 梯度归零</span></div><div class="line">        optimizer.zero_grad()</div><div class="line">        <span class="comment"># 执行反向传播</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 更新参数</span></div><div class="line">        optimizer.step()</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">'Epoch[&#123;&#125;/&#123;&#125;], loss: &#123;:.6f&#125;'</span></div><div class="line">                  .format(epoch + <span class="number">1</span>, num_epochs, loss.data[<span class="number">0</span>]))</div><div class="line"></div><div class="line">    model.eval()</div><div class="line">    predict = model(Variable(x_train))</div><div class="line">    predict = predict.data.numpy()</div><div class="line">    plt.plot(x_train.numpy(), y_train.numpy(), <span class="string">'ro'</span>, label=<span class="string">'Original data'</span>)</div><div class="line">    plt.plot(x_train.numpy(), predict, label=<span class="string">'Fitting Line'</span>)</div><div class="line">    <span class="comment"># 显示图例</span></div><div class="line">    plt.legend()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2017-12-18</span><i class="fa fa-tag"></i><a href="/tags/DL/" title="DL" class="tag">DL </a><a href="/tags/pytorch/" title="pytorch" class="tag">pytorch </a><a href="/tags/深度学习/" title="深度学习" class="tag">深度学习 </a></div></div></div></div><div class="share"><div class="evernote"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"><a href="http://twitter.com/home?status=,http://yoursite.com/child/2017/12/18/PyTorch实践-1-回归/,Heng Ding (丁恒),PyTorch实践-(1)回归,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a role="navigation" href="/2017/08/18/置顶/" title="置顶贴(博文搬迁中,大量干货即将来袭)" class="btn">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>